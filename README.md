# ğŸš€ HackTheData: Desvendando Insights com um Pipeline ETL Robusto

## âœ¨ VisÃ£o Geral do Projeto

Bem-vindo ao repositÃ³rio do **HackTheData**, um projeto inovador desenvolvido durante uma hackathon com o objetivo de democratizar o acesso a dados dispersos e brutos, transformando-os em informaÃ§Ãµes estratÃ©gicas para Business Intelligence. Este projeto consiste em uma **API backend em Python com Flask**, atuando como um **pipeline ETL (Extract, Transform, Load)** de alta performance, projetado para alimentar dashboards dinÃ¢micos no Power BI. ğŸ“ˆğŸ“Š

Nosso foco principal foi a construÃ§Ã£o de uma soluÃ§Ã£o que nÃ£o apenas integra dados de fontes heterogÃªneas (Google Sheets, SharePoint, e atÃ© mesmo imagens via OCR), mas que tambÃ©m os processa de forma inteligente com **Pandas**, garantindo a qualidade e a prontidÃ£o para anÃ¡lise. O resultado sÃ£o endpoints RESTful otimizados, prontos para consumo por ferramentas de BI, demonstrando proficiÃªncia em engenharia de dados, automaÃ§Ã£o e entrega de valor de negÃ³cio. ğŸš€

---

## ğŸ“Œ Badges de Tecnologia

![Python](https://img.shields.io/badge/Python-3.9%2B-blue.svg) 
![Flask](https://img.shields.io/badge/Flask-2.x-orange.svg) 
![Pandas](https://img.shields.io/badge/Pandas-2.x-green.svg) 
![RESTful API](https://img.shields.io/badge/API-RESTful-success.svg) 
![Power BI](https://img.shields.io/badge/Power%20BI-Data%20Visualization-yellow.svg)

---

## ğŸ¯ O Desafio e a SoluÃ§Ã£o: Engenharia de Dados na PrÃ¡tica

O **HackTheData** nasceu da necessidade de unificar e tratar dados crÃ­ticos de negÃ³cio que estavam fragmentados em diversas fontes, muitas vezes em formatos nÃ£o estruturados ou semi-estruturados. Nosso desafio foi criar um fluxo automatizado que garantisse a **confiabilidade, consistÃªncia e acessibilidade** desses dados para a tomada de decisÃµes estratÃ©gicas.

### ğŸ§ª As Etapas do Pipeline ETL: Uma Abordagem Detalhada

Cada fase do nosso pipeline ETL foi cuidadosamente projetada para garantir a mÃ¡xima eficiÃªncia e qualidade dos dados:

#### ğŸ“¥ 1. Extract (ExtraÃ§Ã£o): Conectando Fontes HeterogÃªneas

Nesta etapa, focamos na coleta de dados de diversas origens, demonstrando a capacidade de integraÃ§Ã£o com sistemas variados:

*   **Google Sheets (Receita, Despesas, PLR):** UtilizaÃ§Ã£o da API oficial do Google para extrair dados financeiros e de desempenho, garantindo a autenticaÃ§Ã£o segura via **Contas de ServiÃ§o** e o acesso programÃ¡tico a planilhas dinÃ¢micas. Isso assegura que as informaÃ§Ãµes mais recentes estejam sempre disponÃ­veis. â˜ï¸ğŸ“„
*   **SharePoint (Excel de Clientes e Parcelas):** ImplementaÃ§Ã£o de lÃ³gica para download e leitura de arquivos Excel hospedados no SharePoint, superando desafios de autenticaÃ§Ã£o e acesso a ambientes corporativos. Isso permite a integraÃ§Ã£o de dados de sistemas internos de gestÃ£o. ğŸ¢ğŸ’¾
*   **OCR de Imagens (Metas de Campanha):** Uma abordagem inovadora para extrair dados de fontes nÃ£o convencionais. AtravÃ©s de tÃ©cnicas de **Optical Character Recognition (OCR)**, somos capazes de ler e digitalizar informaÃ§Ãµes contidas em imagens (como screenshots de dashboards ou relatÃ³rios), transformando dados visuais em dados estruturados. ğŸ“¸â¡ï¸ğŸ“

**DomÃ­nio TÃ©cnico:** IntegraÃ§Ã£o de APIs (Google Sheets API), manipulaÃ§Ã£o de arquivos em ambientes corporativos (SharePoint), processamento de imagem e OCR para extraÃ§Ã£o de dados nÃ£o estruturados.

#### ğŸ”„ 2. Transform (TransformaÃ§Ã£o): Refinando e Enriquecendo os Dados

A fase de transformaÃ§Ã£o Ã© onde os dados brutos sÃ£o limpos, padronizados e enriquecidos, tornando-os aptos para anÃ¡lise. Utilizamos a biblioteca **Pandas** para operaÃ§Ãµes de alta performance:

*   **ConversÃ£o de Valores MonetÃ¡rios:** Tratamento de formatos numÃ©ricos e monetÃ¡rios para garantir consistÃªncia e precisÃ£o nos cÃ¡lculos financeiros. ğŸ’°â¡ï¸ğŸ”¢
*   **PadronizaÃ§Ã£o de Datas:** ConversÃ£o de diferentes formatos de data para um padrÃ£o unificado, essencial para anÃ¡lises temporais e sÃ©ries histÃ³ricas. ğŸ“…â¡ï¸ğŸ—“ï¸
*   **Limpeza de Dados Nulos e Irrelevantes:** ImplementaÃ§Ã£o de estratÃ©gias para identificar e tratar valores ausentes ou dados que nÃ£o agregam valor Ã  anÃ¡lise, garantindo a integridade do dataset. ğŸ§¹ğŸ—‘ï¸
*   **ReestruturaÃ§Ã£o de Tabelas:** AplicaÃ§Ã£o de pivoteamento, desnormalizaÃ§Ã£o e outras tÃ©cnicas para modelar os dados em um formato otimizado para consumo em dashboards de BI, facilitando a criaÃ§Ã£o de relatÃ³rios e mÃ©tricas. ğŸ—ï¸

**DomÃ­nio TÃ©cnico:** ManipulaÃ§Ã£o e limpeza de dados com Pandas (DataFrame operations, data type conversions, handling missing values), aplicaÃ§Ã£o de regras de negÃ³cio, modelagem de dados para BI.

#### ğŸ“¤ 3. Load (Carga): Disponibilizando Dados para Consumo

A etapa final do ETL consiste em disponibilizar os dados transformados de forma acessÃ­vel e eficiente para as ferramentas de BI:

*   **ExposiÃ§Ã£o via API Flask:** Os dados limpos e estruturados sÃ£o expostos atravÃ©s de endpoints RESTful, permitindo que o Power BI (ou qualquer outra ferramenta de BI/aplicaÃ§Ã£o) consuma essas informaÃ§Ãµes sob demanda. ğŸŒğŸ”—
*   **Formato JSON Otimizado:** A escolha do formato JSON para a saÃ­da da API garante a interoperabilidade e a facilidade de integraÃ§Ã£o com diversas plataformas. ğŸ“¦

**DomÃ­nio TÃ©cnico:** Desenvolvimento de APIs RESTful com Flask, serializaÃ§Ã£o de dados para JSON, design de endpoints para consumo por ferramentas de BI.

---

## ğŸ“Š Arquitetura do Sistema

Nosso sistema foi projetado com uma arquitetura clara e modular, garantindo escalabilidade e manutenibilidade:

![Arquitetura do Pipeline ETL](https://private-us-east-1.manuscdn.com/sessionFile/PVOa7wBAt4M65kts0N73c5/sandbox/7o0gcwmPnSOV0XoFtfZYnI-images_1755708002801_na1fn_L2hvbWUvdWJ1bnR1L2NvbXBsZXRlX3BpcGVsaW5lX2RpYWdyYW0.png?Policy=eyJTdGF0ZW1lbnQiOlt7IlJlc291cmNlIjoiaHR0cHM6Ly9wcml2YXRlLXVzLWVhc3QtMS5tYW51c2Nkbi5jb20vc2Vzc2lvbkZpbGUvUFZPYTd3QkF0NE02NWt0czBONzNjNS9zYW5kYm94LzdvMGdjd21QblNPVjBYb0Z0ZlpZbkktaW1hZ2VzXzE3NTU3MDgwMDI4MDFfbmExZm5fTDJodmJXVXZkV0oxYm5SMUwyTnZiWEJzWlhSbFgzQnBjR1ZzYVc1bFgyUnBZV2R5WVcwLnBuZyIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc5ODc2MTYwMH19fV19&Key-Pair-Id=K2HSFNDJXOU9YS&Signature=kzmMlNr9VnvQPsrnJL5pqe2XGmV4DVO3MOHIBCwrMF5zL3Ipk247gz6-9jbaRgyp-FA~Q1djm-rn5Celj~Gieu5udhbtoP89EcxHyn2C2~U7RKsHcCe--z8mqB~6bSz4OoXhantk5sHILB9cHaptdMr61aqXjQpLkfQU-MkeuctlfRE8rW6vKWziUlFujzAcV9TUySGDFIz9cySEwj-G1bCRHTNf0UG3GUIHEo2F~OETHYYlMvtyhUBVCRk7o-v9fWGL1kWrSb-kf5QoAa11SaTr5aRbpZAelfTM-Z8ByRAo18knyJ5-yErqSMy4~GHrXnkO071ul6v5-M3lw~2aOQ__)

*Diagrama de fluxo do pipeline ETL, destacando as fontes de dados, a API Flask e a camada de BI.*

### ğŸ—ï¸ Componentes da Arquitetura

| Camada | Componente | Responsabilidade | Tecnologia |
|:-------|:-----------|:-----------------|:-----------|
| **ğŸ“¥ Fontes de Dados** | Google Sheets | Dados financeiros (Receita, Despesas, PLR) | Google Sheets API |
| | SharePoint | Arquivos Excel de clientes e parcelas | HTTP Requests + openpyxl |
| | OCR de Imagens | ExtraÃ§Ã£o de metas de campanhas | OCR.space API |
| **âš™ï¸ API ETL** | sheets.py | ExtraÃ§Ã£o de dados do Google Sheets | google-api-python-client |
| | sharepoint.py | Download e leitura de arquivos Excel | requests + cloudscraper |
| | sheets_cleaner.py | Limpeza e transformaÃ§Ã£o de dados | Pandas |
| | app.py | Endpoints RESTful e orquestraÃ§Ã£o | Flask |
| **ğŸ“ˆ Camada de BI** | Power BI | VisualizaÃ§Ã£o e dashboards | Power BI Desktop/Service |

---

## ğŸ› ï¸ Tecnologias e Ferramentas Utilizadas

Uma visÃ£o detalhada das tecnologias que impulsionam este projeto:

| Categoria | Tecnologia | VersÃ£o | PropÃ³sito | DomÃ­nio TÃ©cnico Demonstrado |
| :-------- | :--------- | :----- | :-------- | :-------------------------- |
| **Linguagem** | Python | 3.9+ | Linguagem principal para desenvolvimento do pipeline e API. | ProgramaÃ§Ã£o orientada a objetos, manipulaÃ§Ã£o de dados, desenvolvimento web. |
| **Framework Web** | Flask | 2.x | ConstruÃ§Ã£o da API RESTful para exposiÃ§Ã£o dos dados. | Desenvolvimento de APIs, roteamento, tratamento de requisiÃ§Ãµes HTTP. |
| **Processamento de Dados** | Pandas | 2.x | ManipulaÃ§Ã£o, limpeza e transformaÃ§Ã£o de grandes volumes de dados. | Data wrangling, otimizaÃ§Ã£o de performance com DataFrames, agregaÃ§Ã£o e pivoteamento. |
| **IntegraÃ§Ã£o Google** | `google-api-python-client`, `google-auth-oauthlib` | Latest | AutenticaÃ§Ã£o e interaÃ§Ã£o com a Google Sheets API. | OAuth 2.0, consumo de APIs REST externas, gerenciamento de credenciais. |
| **RequisiÃ§Ãµes HTTP** | `requests`, `cloudscraper` | Latest | RealizaÃ§Ã£o de requisiÃ§Ãµes HTTP para SharePoint e outras fontes. | RequisiÃ§Ãµes web, tratamento de sessÃµes, bypass de proteÃ§Ãµes (Cloudflare). |
| **Leitura Excel** | `openpyxl` | Latest | Leitura e manipulaÃ§Ã£o de arquivos `.xlsx`. | Parsing de formatos de arquivo, extraÃ§Ã£o de dados estruturados. |
| **OCR** | `ocr.space` API | N/A | ExtraÃ§Ã£o de texto de imagens (screenshots). | IntegraÃ§Ã£o com serviÃ§os de IA/ML externos, processamento de dados nÃ£o estruturados. |
| **VisualizaÃ§Ã£o** | Power BI | N/A | Ferramenta de Business Intelligence para criaÃ§Ã£o de dashboards. | Conectividade com APIs, modelagem de dados para BI, criaÃ§Ã£o de relatÃ³rios interativos. |

---

## âš™ï¸ Guia de InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

Para colocar o projeto em funcionamento, siga os passos abaixo:

### 1. â¬‡ï¸ Clone o RepositÃ³rio

```bash
git clone <https://github.com/LucasAnalyticsData/Hackaton-2025---Emprega-Dados>

```

### 2. ğŸ Crie e Ative o Ambiente Virtual

Ã‰ altamente recomendÃ¡vel utilizar um ambiente virtual para gerenciar as dependÃªncias do projeto:

**No Windows:**
```bash
python -m venv venv
.\venv\Scripts\activate
```

**No macOS/Linux:**
```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. ğŸ“¦ Instale as DependÃªncias

Com o ambiente virtual ativado, instale todas as bibliotecas necessÃ¡rias:

```bash
pip install -r requirements.txt
```

---

## ğŸ” ConfiguraÃ§Ãµes Essenciais

Para garantir o funcionamento completo do pipeline, algumas configuraÃ§Ãµes sÃ£o necessÃ¡rias:

### Google Sheets API

1.  Acesse o [Google Cloud Console](https://console.cloud.google.com/) e crie um novo projeto.
2.  No painel de APIs e ServiÃ§os, ative a **Google Sheets API**.
3.  Crie uma **Conta de ServiÃ§o** (IAM & Admin -> Contas de ServiÃ§o) e gere uma nova chave JSON. Baixe este arquivo.
4.  Renomeie o arquivo JSON baixado para `credentials_sheets.json` e coloque-o na raiz do seu projeto.
5.  **Importante:** Compartilhe a planilha do Google Sheets que vocÃª deseja acessar com o endereÃ§o de e-mail da conta de serviÃ§o (encontrado no arquivo `credentials_sheets.json`).

### OCR.space API (Opcional, para `read_screenshot`)

1.  Crie uma conta gratuita em [https://ocr.space/](https://ocr.space/).
2.  Obtenha sua **API Key** no painel de controle.
3.  No arquivo `app.py`, localize a linha `ocr_apikey = 'SUA_CHAVE_API_AQUI'` e substitua `'SUA_CHAVE_API_AQUI'` pela sua chave real.

---

## â–¶ï¸ ExecuÃ§Ã£o da AplicaÃ§Ã£o

ApÃ³s a instalaÃ§Ã£o e configuraÃ§Ã£o, vocÃª pode iniciar a API Flask:

Com o ambiente virtual ativado, execute:

```bash
python app.py
```

A aplicaÃ§Ã£o estarÃ¡ acessÃ­vel localmente em: 
[http://127.0.0.1:5000](http://127.0.0.1:5000)

---

## ğŸ”— Endpoints da API: Consumo de Dados

Os dados processados sÃ£o disponibilizados atravÃ©s dos seguintes endpoints RESTful, prontos para serem consumidos pelo Power BI ou outras aplicaÃ§Ãµes:

### 1. `/hackaton/get_excel` (GET) ğŸ“„

*   **DescriÃ§Ã£o:** Extrai, limpa e retorna dados de um arquivo Excel hospedado no SharePoint.
*   **Exemplo de Resposta (JSON):**
    ```json
    {
      "data": "[{\"Coluna1\":\"Valor1\", \"Valor parcela\":150.75, ...}]"
    }
    ```

### 2. `/hackaton/get_sheets` (GET) ğŸ“Š

*   **DescriÃ§Ã£o:** Extrai e trata dados de mÃºltiplas abas (`Receita`, `Despesas`, `PLR`) de uma planilha do Google Sheets, alÃ©m de dados de metas (incluindo OCR de imagens).
*   **Exemplo de Resposta (JSON):**
    ```json
    {
      "receita": [...],
      "despesa": [...],
      "plr": [...],
      "metas": [...],
      "metas_historico": [...]
    }
    ```

### 3. `/hackaton/read_screenshot` (POST) ğŸ“¸

*   **DescriÃ§Ã£o:** Recebe uma imagem em formato base64, realiza OCR para extrair metas de campanha e as processa.
*   **Payload de Exemplo (JSON):**
    ```json
    {
      "screenshot": "data:image/png;base64,iVBORw0KGgoAAAAS..." 
      // ConteÃºdo da imagem em base64
    }
    ```
*   **Resposta de Sucesso (JSON):**
    ```json
    {
      "success": true,
      "message": "Screenshot saved as temp.png"
    }
    ```
*   **Resposta de Erro (JSON):**
    ```json
    {
      "success": false,
      "message": "Meta not found in screenshot"
    }
    ```

---

## ğŸ“ Estrutura do Projeto: Modularidade e OrganizaÃ§Ã£o

O projeto segue uma estrutura modular para facilitar a manutenÃ§Ã£o e o desenvolvimento:

```plaintext
.
â”œâ”€â”€ app.py                 # ğŸŒ Endpoints da API Flask e lÃ³gica principal
â”œâ”€â”€ sheets.py              # ğŸ“„ LÃ³gica de integraÃ§Ã£o com Google Sheets
â”œâ”€â”€ sheets_cleaner.py      # ğŸ§¹ FunÃ§Ãµes de limpeza e transformaÃ§Ã£o de dados das planilhas
â”œâ”€â”€ sharepoint.py          # â˜ï¸ LÃ³gica para download de arquivos Excel do SharePoint
â”œâ”€â”€ config.py              # âš™ï¸ ConfiguraÃ§Ãµes gerais e variÃ¡veis de ambiente
â”œâ”€â”€ credentials_sheets.json # ğŸ”‘ Credenciais de serviÃ§o do Google Sheets (local)
â””â”€â”€ requirements.txt       # ğŸ“¦ Lista de dependÃªncias do projeto
```

---

## ğŸ¤ ContribuiÃ§Ã£o

Sinta-se Ã  vontade para explorar, propor melhorias ou reportar issues. ContribuiÃ§Ãµes sÃ£o sempre bem-vindas! âœ¨

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© open-source e foi desenvolvido exclusivamente para fins educacionais e de demonstraÃ§Ã£o durante a Hackathon. ğŸ“

---

*Desenvolvido com paixÃ£o por dados e engenharia. ğŸ’œ*




## ğŸ§  DomÃ­nio TÃ©cnico e Senioridade Demonstrados

Este projeto Ã© uma vitrine de habilidades essenciais para um Engenheiro de Dados sÃªnior, abrangendo desde a arquitetura de soluÃ§Ãµes atÃ© a implementaÃ§Ã£o de detalhes tÃ©cnicos cruciais:

*   **Engenharia de Dados End-to-End:** Demonstra a capacidade de construir e gerenciar pipelines de dados completos, desde a ingestÃ£o de diversas fontes atÃ© a disponibilizaÃ§Ã£o para consumo final. ğŸ”„
*   **IntegraÃ§Ã£o de Sistemas Complexos:** ExperiÃªncia comprovada na integraÃ§Ã£o com APIs de terceiros (Google Sheets, OCR.space) e sistemas corporativos (SharePoint), lidando com autenticaÃ§Ã£o, formatos de dados variados e tratamento de erros. ğŸ”—
*   **Processamento de Dados em Escala (Pandas):** ProficiÃªncia na utilizaÃ§Ã£o do Pandas para manipulaÃ§Ã£o, limpeza e transformaÃ§Ã£o eficiente de grandes volumes de dados, incluindo otimizaÃ§Ãµes para performance. ğŸš€
*   **Desenvolvimento de APIs Robustas:** Habilidade em projetar e implementar APIs RESTful com Flask, garantindo que os dados sejam expostos de forma segura, performÃ¡tica e facilmente consumÃ­vel por outras aplicaÃ§Ãµes. ğŸŒ
*   **Tratamento de Dados NÃ£o Estruturados (OCR):** InovaÃ§Ã£o na extraÃ§Ã£o de informaÃ§Ãµes de fontes nÃ£o convencionais (imagens), utilizando OCR, o que destaca a capacidade de resolver problemas complexos e pensar fora da caixa. ğŸ“¸
*   **Qualidade e GovernanÃ§a de Dados:** PreocupaÃ§Ã£o com a qualidade dos dados desde a extraÃ§Ã£o atÃ© a transformaÃ§Ã£o, com etapas claras de limpeza, padronizaÃ§Ã£o e validaÃ§Ã£o. âœ…
*   **AutomaÃ§Ã£o e OrquestraÃ§Ã£o:** O projeto, por ser um pipeline ETL, intrinsecamente demonstra a capacidade de automatizar fluxos de trabalho de dados, reduzindo a intervenÃ§Ã£o manual e aumentando a eficiÃªncia. âš™ï¸
*   **ComunicaÃ§Ã£o TÃ©cnica:** A estrutura e a documentaÃ§Ã£o do cÃ³digo (implÃ­cita no HTML original) e deste README refletem a habilidade de comunicar conceitos tÃ©cnicos complexos de forma clara e concisa. ğŸ—£ï¸

Essas competÃªncias sÃ£o fundamentais para atuar em ambientes de dados desafiadores e complexos, onde a capacidade de construir soluÃ§Ãµes escalÃ¡veis e confiÃ¡veis Ã© primordial. ğŸŒŸ

---



## ğŸ§  Motivadores TÃ©cnicos por TrÃ¡s da Arquitetura

Nosso pipeline foi construÃ­do sobre princÃ­pios sÃ³lidos de Engenharia de Dados, garantindo nÃ£o apenas a funcionalidade, mas tambÃ©m a robustez, escalabilidade e manutenibilidade. A tabela abaixo detalha as motivaÃ§Ãµes tÃ©cnicas por trÃ¡s de cada escolha arquitetural:

| âœ… PrincÃ­pio | ğŸ’¡ MotivaÃ§Ã£o TÃ©cnica | DomÃ­nio TÃ©cnico Demonstrado |
| :-------- | :------------------- | :-------------------------- |
| **ğŸ” Schema Enforcement** | Garante que os dados lidos das fontes estejam consistentes com o modelo esperado, prevenindo erros e inconsistÃªncias. | GovernanÃ§a de Dados, Qualidade de Dados, PrevenÃ§Ã£o de Erros |
| **ğŸ§¹ Limpeza de Dados** | RemoÃ§Ã£o de duplicatas e nulos assegura qualidade mÃ­nima logo na camada de ingestÃ£o, otimizando processamentos futuros. | PrÃ©-processamento de Dados, OtimizaÃ§Ã£o de Performance, Qualidade de Dados |
| **â±ï¸ Rastreabilidade** | InclusÃ£o de `data_ingestao` e outros metadados para auditoria e suporte ao Time Travel (se aplicÃ¡vel), permitindo a recuperaÃ§Ã£o de estados anteriores dos dados. | Auditoria de Dados, Versionamento de Dados, RecuperaÃ§Ã£o de Desastres |
| **ğŸ” Upsert via MERGE** | Carregamento incremental e seguro dos dados com `MERGE INTO`, mantendo a integridade e performance em cenÃ¡rios de dados em constante mudanÃ§a. | Processamento Incremental, IdempotÃªncia, OtimizaÃ§Ã£o de IngestÃ£o |
| **ğŸ§¬ EvoluÃ§Ã£o de Schema** | `mergeSchema` garante flexibilidade para alteraÃ§Ãµes estruturais futuras nas fontes de dados sem quebrar o pipeline. | Flexibilidade de Schema, Manutenibilidade, ResiliÃªncia a MudanÃ§as |
| **ğŸ“ Particionamento** | Organiza fisicamente os dados por chaves especÃ­ficas (ex: `data_ingestao`), otimizando a leitura e escrita em grandes volumes. | OtimizaÃ§Ã£o de Armazenamento, Performance de Consulta, Gerenciamento de Dados |
| **ğŸ§  Auto-recuperaÃ§Ã£o** | Registro da tabela no catÃ¡logo evita falhas comuns como `TABLE_OR_VIEW_NOT_FOUND`, garantindo a disponibilidade dos dados. | ResiliÃªncia do Pipeline, Gerenciamento de Metadados, Disponibilidade de Dados |
| **âš™ï¸ OtimizaÃ§Ãµes Delta** | `OPTIMIZE ZORDER BY` e `VACUUM` maximizam performance de consulta e reduzem custo de armazenamento em ambientes Delta Lake. | OtimizaÃ§Ã£o de Data Lakehouse, Gerenciamento de Custo, Performance de Leitura |

---



## ğŸ“ Estrutura do Projeto: Modularidade e OrganizaÃ§Ã£o

O projeto segue uma estrutura modular para facilitar a manutenÃ§Ã£o e o desenvolvimento, com cada componente tendo uma responsabilidade clara:

```plaintext
.
â”œâ”€â”€ app.py                 # ğŸŒ Endpoints da API Flask e lÃ³gica principal de orquestraÃ§Ã£o
â”œâ”€â”€ sheets.py              # ğŸ“„ MÃ³dulo de integraÃ§Ã£o com Google Sheets
â”œâ”€â”€ sheets_cleaner.py      # ğŸ§¹ MÃ³dulo com funÃ§Ãµes de limpeza e transformaÃ§Ã£o de dados das planilhas
â”œâ”€â”€ sharepoint.py          # â˜ï¸ MÃ³dulo para download de arquivos Excel do SharePoint
â”œâ”€â”€ config.py              # âš™ï¸ Arquivo de configuraÃ§Ãµes gerais e variÃ¡veis de ambiente
â”œâ”€â”€ credentials_sheets.json # ğŸ”‘ Arquivo de credenciais de serviÃ§o do Google Sheets (local e seguro)
â””â”€â”€ requirements.txt       # ğŸ“¦ Lista de dependÃªncias do projeto para fÃ¡cil instalaÃ§Ã£o
```

Esta organizaÃ§Ã£o promove a **manutenibilidade**, **escalabilidade** e **colaboraÃ§Ã£o** entre equipes, permitindo que diferentes mÃ³dulos sejam desenvolvidos e testados de forma independente. ğŸ§‘â€ğŸ’»

---

## ğŸ¤ ContribuiÃ§Ã£o

Sua contribuiÃ§Ã£o Ã© muito bem-vinda! Sinta-se Ã  vontade para explorar o cÃ³digo, propor melhorias, abrir **issues** para reportar bugs ou sugerir novas funcionalidades, e enviar **pull requests**. Juntos, podemos aprimorar ainda mais este projeto. âœ¨

---

## ğŸ“„ LicenÃ§a

Este projeto Ã© open-source e foi desenvolvido exclusivamente para fins educacionais e de demonstraÃ§Ã£o durante a Hackathon. Ele serve como um portfÃ³lio prÃ¡tico de habilidades em Engenharia de Dados e Desenvolvimento Backend. ğŸ“

---

*Desenvolvido com paixÃ£o por dados e engenharia. ğŸ’œ*


